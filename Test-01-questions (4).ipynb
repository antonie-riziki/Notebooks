{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test 01 (Midterm Exam): Scikit-Learn, Classification, Regression\n",
    "\n",
    "Please answer the following questions using Python and/or written responses in the notebook\n",
    "cells below.  This tests covers the materials we have looked at so far in this class, including\n",
    "Python and the scientific python stack, supervised learning for classification and regression\n",
    "problems, using the scikit-learn ML library framework, and linear and logistic regression.\n",
    "You should fill out the cells to answer the questions, and submit your working notebook\n",
    "to the correct submission folder in MyLeoOnline/D2L before the deadline for this test.\n",
    "Please make sure that your notebook runs all cells cleanly from top to bottom when a run\n",
    "all is performed, as you have been doing for your assignments.  Expected output is given in\n",
    "many places for this notebook, that depends on a random number seed being set and the cells executed\n",
    "in order.  So you need to ensure you are exeucuting cells in order in order to get the correct results.\n",
    "Please ensure you use markdown cells for any written responses you are asked to submit for this test.\n",
    "Please use standard Python PEP coding style standards for your work, and provide Python doc comments\n",
    "for any functions you create in this notebook.\n",
    "\n",
    "As a reminder, all work on tests and assignments are required to be the sole product of the individual submitting the work.  You may not work in a group or look at other current or past student work or solutions while working on your test.  Copied work may receive a 0 grade for this midterm exam and may be subject to disciplinary actions.\n",
    "\n",
    "**Due: Friday 10/21/2022**\n",
    "\n",
    "Please add your name and the last 5 digits of your CWID here for my reference and in case a notebook\n",
    "gets accidently misplaced or copied while grading.\n",
    "\n",
    "Name: Jane Student\n",
    "\n",
    "CWID-5: (last 5 digits of cwid)\n",
    "\n",
    "In the following cells, we import some common libraries, including a few from the scikit-learn\n",
    "framework, and set some plotting and visualization defaults.  However, you may need to add in additional\n",
    "imports to your notebook for your work on the questions for this test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "# By convention, we often just import the specific classes/functions\n",
    "# from scikit-learn we will need to train a model and perform prediction.\n",
    "# Here we include all of the classes and functions you should need for this\n",
    "# assignment from the sklearn library, but there could be other methods you might\n",
    "# want to try or would be useful to the way you approach the problem, so feel free\n",
    "# to import others you might need or want to try\n",
    "\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.datasets import make_blobs\n",
    "import statsmodels.api as sm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# notebook wide settings to make plots more readable and visually better to understand\n",
    "np.set_printoptions(suppress=True)\n",
    "plt.style.use('seaborn-darkgrid')\n",
    "plt.rc('axes', labelsize=14)\n",
    "plt.rc('xtick', labelsize=12)\n",
    "plt.rc('ytick', labelsize=12)\n",
    "plt.rc('figure', titlesize=24)\n",
    "plt.rc('legend', fontsize=14)\n",
    "plt.rcParams['figure.figsize'] = (12.0, 8.0) # default figure size if not specified in plot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Numpy and Generation of Polynomial Function Data\n",
    "---------------------------\n",
    "\n",
    "To start this test, we are first going to create a function that can\n",
    "randomly generate a set of data of a single parameter x, but where the\n",
    "function or label of the parameter x is some nonlinear polynomial combination\n",
    "of x.  The function will also add in noise to the randomly generated data,\n",
    "to make it more difficult to find and fit a model to the data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NumPy Practice\n",
    "\n",
    "But first, perform the following tasks.  You will use the following to\n",
    "create the function that generates a random polynomial dataset.\n",
    "\n",
    "In the next cell, set the `NumPy` random seed to 42, so that all of\n",
    "your following work generating random numbers will get the expected result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set NumPy random seed to 42 here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate a NumPy array of shape (5,) of randomly\n",
    "generated numbers in the range from [-1, 1].  Use the\n",
    "`uniform()` function of the `NumPy` random library to do this.  Call the\n",
    "array `x`.  If you set your seed and generate it correctly, you should\n",
    "get the following results:\n",
    "\n",
    "```python\n",
    "> print(x)\n",
    "[-0.25091976  0.90142861  0.46398788  0.19731697 -0.68796272]\n",
    "\n",
    "> print(x.shape)\n",
    "(5,)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# m is the number of samples, use m = 5 sample size for the cells in this\n",
    "# question section\n",
    "m = 5\n",
    "\n",
    "# generate the array of m = 5 random numbers here, all values in the range [-1, 1]\n",
    "\n",
    "# uncomment these, make sure you get the shape and values expected\n",
    "#print(x)\n",
    "#print(x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reshape the vector called `x` so that it is a column matrix, e.g.\n",
    "a matrix with 5 rows and 1 column, so a shape of (5, 1).  Make sure\n",
    "to reassign reshaped result back into x to use in next few cells.\n",
    "\n",
    "```python\n",
    "> print(x)\n",
    "[[-0.25091976]\n",
    " [ 0.90142861]\n",
    " [ 0.46398788]\n",
    " [ 0.19731697]\n",
    " [-0.68796272]]\n",
    "\n",
    "> print(x.shape)\n",
    "(5, 1)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape x into a column matrix of the required shape.  Make sure to\n",
    "# reassign reshaped array back into x so that variable x has the desired shape\n",
    "\n",
    "# uncomment these and make sure you get the exact results shown\n",
    "#print(x)\n",
    "#print(x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an array called `theta` of shape `(3,)` (a vector) with\n",
    "the following values.  Crete a variable named degree which should\n",
    "be determined by querying the shape of `theta` and subtracting 1 from\n",
    "its shape.\n",
    "\n",
    "```python\n",
    "> print(theta)\n",
    "[ 3 -4  5]\n",
    "\n",
    "> print(theta.shape)\n",
    "(3,)\n",
    "\n",
    "> print(degree)\n",
    "2\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the array called theta with the values and shape indicated\n",
    "\n",
    "# uncomment these and make sure you get the exact results shown\n",
    "#print(theta)\n",
    "#print(theta.shape)\n",
    "\n",
    "# create a variable named degree.  Make sure that you query theta shape and subtract 1\n",
    "# to determine the degree, which should be 2 here since theta is shaped (3,)\n",
    "\n",
    "# uncomment this and make sure you get the expected result\n",
    "#print(degree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using SciKit `PolynomialFeatures` class, create a new array `X` of features from the original `x`, but generating the `degree=2` polynomial features.\n",
    "Make sure you include the bias term.  The resulting `X` should look like\n",
    "this:\n",
    "\n",
    "```python\n",
    "> print(X)\n",
    "[[ 1.         -0.25091976  0.06296073]\n",
    " [ 1.          0.90142861  0.81257354]\n",
    " [ 1.          0.46398788  0.21528476]\n",
    " [ 1.          0.19731697  0.03893399]\n",
    " [ 1.         -0.68796272  0.4732927 ]]\n",
    "\n",
    "> print(X.shape)\n",
    "(5, 3)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a polynomial features instance of the indicated degree\n",
    "\n",
    "# fit/transform the single feature x column matrix into a degree 2 set of\n",
    "# polynomial features\n",
    "\n",
    "# uncomment these and make sure you get exactly the same values and shape\n",
    "# expected here for the X polynomial features\n",
    "#print(X)\n",
    "#print(X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform a matrix multiplication to multiply the polynomial features `X`\n",
    "times the `theta` parameters (**Hint**: NumPy `dot()` function, or overloaded\n",
    "operator that performs matrix multiplication in Python 3 NumPy).\n",
    "The result will be the `y` value of the\n",
    "quadratic function $y = 3 - 4x + 5x^2$ for the 5 sampled `x` values you generated.\n",
    "If you perform the matrix multiplication correctly,  you should get an array named `y`\n",
    "with the following values:\n",
    "\n",
    "```python\n",
    "> print(y)\n",
    "[4.31848268 3.45715327 2.22047225 2.40540206 8.11831439]\n",
    "\n",
    "> print(y.shape)\n",
    "(5,)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform matrix multiplication of X polynomial features and theta parameters\n",
    "\n",
    "# uncomment these and make sure you get exactly the same resulting y\n",
    "# labels for this quadratic function\n",
    "#print(y)\n",
    "#print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add some gaussian (normally distributed) noise to the y labels.  Use\n",
    "a mean (`mu` $\\mu$) of 0.0 and a standard deviation (`sigma` $\\sigma$) of 0.1 for the\n",
    "generated noise.  Use the `NumPy` random library function named\n",
    "`normal()` to generate this noise with the indicated mean and\n",
    "standard deviation, and of the correct shape `m = 5` so `(5,)` to add\n",
    "the noise to `y`.  The result should be saved back to the variable `y`.\n",
    "\n",
    "If you generate this noise as asked for, you should now have these exact\n",
    "values in your `y` variable.\n",
    "\n",
    "```python\n",
    "> print(y)\n",
    "[4.34638681 3.5582048  2.16238443 2.35288508 8.06117637]\n",
    "\n",
    "> print(y.shape)\n",
    "(5,)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add in normally distributed noise to the y labels with a mean of mu=0.0\n",
    "# and a standard deviation of sigma=0.1\n",
    "mu = 0.0\n",
    "sigma = 0.1\n",
    "\n",
    "# uncomment these lines and make sure you are getting the expected noisy y labels now\n",
    "#print(y)\n",
    "#print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Polynomial Dataset Generator Function\n",
    "\n",
    "At this point you have all of the pieces you need to create a function\n",
    "to generate a random polynomial dataset of any degree.  The\n",
    "function signature is given to you here in the next cell.  Use the work\n",
    "you did in the previous cells to generate a set of `x` features randomly\n",
    "on the interval [-1, 1], determine the degree of the polynomial from\n",
    "the `theta` array, generate the polynomial features, generate the `y`\n",
    "labels and then add some gaussian noise to the `y` labels.  This function\n",
    "returns the resulting randomly generated `x` and `y` arrays.\n",
    "\n",
    "The cell after the function implementation calls your function to generate\n",
    "a random dataset.  It expects that `m = 5` and the `theta` array were defined above.\n",
    "It also expects that you ask for a data set with random noise where `mu = 0.0` and \n",
    "`sigma = 0.1`.  Invoke your function and generate random features `x` and\n",
    "the corresponding noisy labels `y`. If you have set your seed as asked for and run\n",
    "these cells sequentially, you should get the following result in `x` and `y`\n",
    "after running your function:\n",
    "\n",
    "```python\n",
    "> print(x)\n",
    "[[ 0.04951286]\n",
    " [-0.13610996]\n",
    " [-0.41754172]\n",
    " [ 0.22370579]\n",
    " [-0.72101228]]\n",
    "> print(x.shape)\n",
    "(5, 1)\n",
    "\n",
    "> print(y)\n",
    "[2.72179788 3.54626705 5.40064195 2.50196312 8.46076501]\n",
    "> print(y.shape)\n",
    "(5,)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implement the required function here\n",
    "def make_polynomial_dataset(m, theta, mu=0.0, sigma=1.0):\n",
    "    \"\"\"Make a randomly generated artificial regression dataset based on a polynomial function.  The degree of the\n",
    "    polynomial is determined by the number of parameters given in the theta array.  For example a quadratic\n",
    "    (squared) function of the form 3 - 4x + 5x^2 would be generated by passing in theta = [3, -4, 5].\n",
    "    The m parameter controls the number of random samples of x to be generated in that interval. \n",
    "    x values are sampled randomly using a uniform distribution in the interval from [-1.0, 1.0]. \n",
    "    We generate y target labels according to the indicated polynomial function, and add in gaussian noise\n",
    "    using a mean of mu and a standard deviation of sigma, indicated by additional default parameters\n",
    "    to this function.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    m - Number of random samples to artificially generate (an integer value >= 1)\n",
    "    theta - terms of the polynomial to generate.  theta.size-1 indicates the degree of the polynomial\n",
    "       to generate, and the parameters in theta are of the form [x_0, x_1, x_2, ..., x_n].  So a \n",
    "       theta parameter of [3, -4, 5] indicates a degree 2 polynomial with parameters y = 3 - 4x + 5x^2\n",
    "    mu, sigma - terms controlling the amount of random noise added to each artifically generated\n",
    "       y target label.  mu is the mean of the gaussian noise to generate and sigma is the standard\n",
    "       deviation of the noise to be generated and added to the sample labels.\n",
    "       \n",
    "    Returns\n",
    "    -------\n",
    "    x,y - Returns a tuple of the randomly generated x features, and the noisy y regression labels.  Both\n",
    "      will be NumPy arrays with m elements in them.  x is a column matrix of shape (m,1) while y is a vector\n",
    "      with shape (5,)\n",
    "    \"\"\"\n",
    "    # generate x samples uniformly over range -1 to 1\n",
    "    \n",
    "    # reshape x as a column array to perform matrix multiplication directly\n",
    "    \n",
    "    # determine polynomial degree\n",
    "    \n",
    "    # create polynomial features from original x samples\n",
    "\n",
    "    # use theta parameters to generate y labels from sampled polynomial features\n",
    "    \n",
    "    # add noise to the regression labels with mean mu and standard deviation sigma\n",
    "    \n",
    "    # return the random features and their regression targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# invoke your function here and check that the resulting x and y match the expected results\n",
    "\n",
    "# Uncomment the following and check your generated x and y values are what are expected at this point\n",
    "#print(x)\n",
    "#print(x.shape)\n",
    "#print(y)\n",
    "#print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Regression Data for Part 2 and Visualize It\n",
    "\n",
    "Next we will use the function you just created to generate a random dataset governed by the\n",
    "following polynomial:\n",
    "\n",
    "$$\n",
    "y = 3 + 5x + 1x^2 - 3x^3 + 2x^4 -3x^5\n",
    "$$\n",
    "\n",
    "Generate a sample size of `m = 1000` data points this time using the `theta` parameters indicated in this cells equation for the\n",
    "5th degree polynomial.  Use a mean of 0 but a standard deviation of 1.0 this time for the noise added to these labels.\n",
    "\n",
    "If you have implemented your function and generated the asked for polynomial, you should get the following values in\n",
    "the first 5 features and labels of this noisy dataset:\n",
    "\n",
    "```python\n",
    "> print(x[:5])\n",
    "[[-0.60065244]\n",
    " [ 0.02846888]\n",
    " [ 0.18482914]\n",
    " [-0.90709917]\n",
    " [ 0.2150897 ]]\n",
    "\n",
    "> print(x.shape)\n",
    "(1000, 1)\n",
    "\n",
    "> print(y[:5])\n",
    "[1.53083608 3.17284304 4.87933597 4.2069929  4.19087997]\n",
    "\n",
    "> print(y.shape)\n",
    "(1000,)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# declare the correct value for m, theta, mu and sigma here\n",
    "\n",
    "# invoke your function to generate the degree 5 dataset\n",
    "\n",
    "# uncomment the following and make sure you seem to have gotten the expected results so far, your values\n",
    "# should exactly match these if you have set the random seed as asked for and have run the cells sequentially\n",
    "# to this point\n",
    "#print(x[:5])\n",
    "#print(x.shape)\n",
    "#print(y[:5])\n",
    "#print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next cell you are to visualize this generated dataset that you will be using\n",
    "in Part 2 of this test to perform some linear regressions.\n",
    "\n",
    "- Plot the raw data as scatter plot points.\n",
    "- Plot the true function as a black line.  Make sure that you use\n",
    "  a different linearly spaced grid of x points when plotting the true\n",
    "  function as discussed in class (e.g. you should not be using the `x` variable with the randomly generated features here.\n",
    "- Label your figure axis, though this data is only the x feature and the y label (it is made up data).\n",
    "  But also label your figure elements, e.g. use a legend to indentify the noisy data points and the\n",
    "  true polynomial function line.\n",
    "\n",
    "Your figure should look similar to the following if your data has been generated correctly and you add the asked for\n",
    "elements to the plot.\n",
    "\n",
    "![Degree 5 random polynomial dataset](test-01-question-01-dataset.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot randomly generated dataset here first as scatter plot\n",
    "\n",
    "# create a different set of x grid values, linearly spaced from -1 to 1\n",
    "\n",
    "# create the true function y values, probably need to use PolynomialFeatures\n",
    "# again here\n",
    "\n",
    "# plot the true function as a solid black line on your figure\n",
    "\n",
    "# add axis labels and legend here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, while we are at it, generate a second set of data named `x_test` and `y_test` this time, with `m = 1000` samples again, using\n",
    "the same polynomial function and the same `mu` and `sigma` values to add noise to the label.  We will use this data to test how well\n",
    "fitted models perform on data they have not been fitted with. Create your `x_test` and `y_test` in the next cell for later use.  You will get\n",
    "the following results for this test data:\n",
    "\n",
    "```python\n",
    "> print(x_test[:5])\n",
    "[[ 0.01482388]\n",
    " [ 0.74844505]\n",
    " [-0.01290684]\n",
    " [ 0.40451753]\n",
    " [ 0.98563368]]\n",
    "\n",
    "> print(x_test.shape)\n",
    "(1000, 1)\n",
    "\n",
    "> print(y_test[:5])\n",
    "[3.97139565 7.24251709 3.58716089 3.87101553 4.92251277]\n",
    "\n",
    "> print(y_test.shape)\n",
    "(1000,)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a set of test data of the same size as our training data\n",
    "\n",
    "# uncomment these and make sure that you get the expected values for this data as well\n",
    "#print(x_test[:5])\n",
    "#print(x_test.shape)\n",
    "#print(y_test[:5])\n",
    "#print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Linear Regression on Polynomial Function\n",
    "------------------------\n",
    "\n",
    "For the next part of this exam you will be performing a sequence of linear regressions on the artificial data set you just generated. \n",
    "You will be using your `x` and `y` NumPy arrays to perform all of the training of your regression models in the following.  Hold off using\n",
    "the `x_test` and `y_text` arrays until asked to do so."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit a Linear Regression\n",
    "\n",
    "In the next cell(s) find a best fit linear regression (line) to your artificial dataset, the `x` and `y`, fitting a line to all of the\n",
    "data you have.  Please use Scikit-Learn Library objects to fit your linear model.  Report all of the following for the fitted model on\n",
    "all of the data.\n",
    "\n",
    "1. Make a plot of your noisy data with your fitted model line drawn with the data.\n",
    "2. Report the intercept and slope coefficients of your fitted line.\n",
    "3. Find and report the $R^2$ measure, which is the goodness of the linear regression fit to the given data.\n",
    "4. Calculate and report the MSE and RMSE cost of your fitted model on all of the data.\n",
    "\n",
    "You should get the following values for the parameters, $R^2$, MSE and RMSE if you have fitted the model\n",
    "correctly and your random seed was set correctly at this point:\n",
    "\n",
    "```\n",
    "Intercept:  3.855189716230805\n",
    "Slope    :  [1.91250359]\n",
    "R^2      :  0.33425755500437426\n",
    "MSE      :  2.504874453540251\n",
    "RMSE     :  1.582679517002811\n",
    "```\n",
    "\n",
    "The fitted linear model should match the following figure when you generate it.\n",
    "\n",
    "![Linear Model Fit to Noisy 5th Degree Dataset](test-01-question-01-linearfit.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the linear regression model and fit the data using Scikit-Learn\n",
    "\n",
    "# 1. Make a scatter plot with the fitted regression line model\n",
    "# and plot the linear regression model on the data\n",
    "\n",
    "# 2. report the intercept and slope\n",
    "\n",
    "# 3. Report the R^2 fit determined by scikit-learn LinearRegression instance\n",
    "\n",
    "# 4. Report the RMSE cost of the final fitted model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model is of course not a really good fit as the true function is not a linear function that is\n",
    "generating this data.  You should make a note of the $R^2$ score and the RMSE cost of this linear\n",
    "model's fit to this data and compare it to later better models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit a 5th Order Regression\n",
    "\n",
    "As you know this artificial data set is actually generated from a 5th degree polynomial with some random noise in\n",
    "the data mapping x to the dependent variable y.  If we knew or suspected this was a degree 5 polynomial\n",
    "function, we could fit a polynomial of that degree to the data and see how good our fit is.\n",
    "\n",
    "For example, here I will just show you the result of using `np.polyfit()` to fit a 5th order\n",
    "polynomial to our noisy data.\n",
    "\n",
    "```python\n",
    "# fit a degree 5 polynomial to the data\n",
    "x = x.reshape((m,))\n",
    "theta = np.polyfit(x, y, 5) \n",
    "\n",
    "# report the fitted parameters, polyfit returns parameters from highest to lowest,\n",
    "# so flip them to have in order we have been using\n",
    "theta = np.flip(theta)\n",
    "print(theta)\n",
    ">>> [ 3.0735744   4.92857937  1.22891594 -2.74466058  1.71305691 -3.07157513]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if your x and y were generated correctly, you should be able to uncomment these and get the same linear\n",
    "# fit from polyfit here\n",
    "# fit a degree 5 polynomial to the data\n",
    "#x = x.reshape((m,))\n",
    "#theta = np.polyfit(x, y, 5) \n",
    "\n",
    "# report the fitted parameters, polyfit returns parameters from highest to lowest,\n",
    "# so flip them to have in order we have been using\n",
    "#theta = np.flip(theta)\n",
    "#print(theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall once again the true parameters you should be using that generated your random nonlinear dataset:\n",
    "\n",
    "$$\n",
    "y = 3 + 5x + 1x^2 - 3x^3 + 2x^4 -3x^5\n",
    "$$\n",
    "\n",
    "You should find that, though of course the fit is not exact because of the added noise, the\n",
    "fitted parameters match pretty well with the true function parameters used to generate the data, despite\n",
    "a relatively large amount of noise added to the output labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usually, however, you don't know the shape or order of the underlying function that controls your data.\n",
    "But let's fit a 5th degree polynomial using Scikit-Learn `LinearRegression` and `PolynomialFeatures`.  You\n",
    "should get the same fit as we just obtained with the NumPy `polyfit()` function (e.g. slope and intercept parameters should match).\n",
    "\n",
    "In the next cell use the `PolynomialFeatures` function from `scikit-learn` to generate\n",
    "all combinations of features for a `degree=5` polynomial features.  You do not need the bias term\n",
    "when generating polynomial features to be used by a `scikit-learn` transformer.  Since\n",
    "you generated the data using a known seed, your resulting set of features should have the following\n",
    "as its first 5 rows of sample values:\n",
    "\n",
    "```python\n",
    "> print(X.shape)\n",
    "(1000, 5)\n",
    "\n",
    "> print(X[:5,:])\n",
    "[[-0.60065244  0.36078335 -0.2167054   0.13016462 -0.0781837 ]\n",
    " [ 0.02846888  0.00081048  0.00002307  0.00000066  0.00000002]\n",
    " [ 0.18482914  0.03416181  0.0063141   0.00116703  0.0002157 ]\n",
    " [-0.90709917  0.82282891 -0.74638743  0.67704742 -0.61414916]\n",
    " [ 0.2150897   0.04626358  0.00995082  0.00214032  0.00046036]]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a polynomial features of degree 5 (include the bias term) and fit/transform it to the raw\n",
    "# x features of your dataset\n",
    "\n",
    "# Uncomment these and check that the shape should be (1000,5) after transforming to add the\n",
    "# higher order polynomial features,  and the first 5 samples should look like the following\n",
    "#print(X.shape)\n",
    "#print(X[:5,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given this set of features, use a `scikit-learn` `LinearRegression` transformer to fit a regressor to this expanded set of\n",
    "features, and report the coefficients you end up finding for your fit.  As before\n",
    "report your intercept and coefficients, the R^2 fit score and the MSE and RMSE of the\n",
    "cost function when fit to all of the data using this regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit a new linear regression to the expanded feature matrix\n",
    "\n",
    "# report the intercept and slope, R^2 score and MSE/RMSE here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here you should find that intercept corresponds to the bias or intercept term.  The coefficients are arranged\n",
    "in the reverse order, from the lowest to the highest, so the coefficient at index 0 is the $x^1$ term, and the\n",
    "last coefficient returned from the model is the $x^5$ term. You should also find that this fit is exactly the same\n",
    "as the one from `polyfit()` above if you performed it correctly.\n",
    "\n",
    "Also take a moment to examine the $R^2$ score and RMSE fit cost. You should find that $R^2$ is much higher now,\n",
    "double what we got for the linear fit.  Also the RMSE cost should have improved quite a bit (it should be around \n",
    "1.0 for this model). Keep these values in mind as we fit the next models.\n",
    "\n",
    "Plot this fitted model in the next cell.  Show the raw noisy data as a scatter plot, and plot both the\n",
    "true function (as a red dashed line) and your `scikit-learn` fitted 5th degree linear regression (as a black solid line).\n",
    "Your plot should look similar the following figure if you have fit your data correctly and all of your data matches the\n",
    "expected results so far.\n",
    "\n",
    "![Fit of 5th order polynomial to cubic data](test-01-question-02-5thorderfit.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create scatter plot here of the noisy dataset\n",
    "\n",
    "# plot the scikit-learn fit of your model using a black line\n",
    "\n",
    "# plot the true function somehow on the graph as a red dashed line here\n",
    "\n",
    "# make sure you label your axis and create a legend showing your plot elements here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also check the performance of this model on the `x_test` and `y_test` held back test sets.  Report the MSE / RMSE on the test data\n",
    "for this fitted model here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine MSE / RMSE here on test data for the 5th degree fitted model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overfit Noisy Data using 50th Degree Polynomial\n",
    "\n",
    "The function you used in assignment 03 to plot learning curves of a model has been copied once again for you in the next cell.  You will use it\n",
    "in the following work to fit and regularize a regression model to this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you will use this function once again to visualize the fit performance of models to your noisy data in the next few tasks\n",
    "def plot_learning_curves(model, X, y):\n",
    "    \"\"\"Plot learning curves obtained with training the given scikit-learn model\n",
    "    with progressively larger amounts of the training data X.\n",
    "    \n",
    "    Nothing is returned explicitly from this function, but a plot will be created\n",
    "    and the resulting learning curves displayed on the plot.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    model - A scikit-learn estimator model to be trained and evaluated.\n",
    "    X - The input training data\n",
    "    y - The target labels for training\n",
    "    \"\"\"\n",
    "    # we actually split out 20% of the data solely for validation, we train on the other 80%\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2)\n",
    "    \n",
    "    # keep track of history of the training and validation cost / error function\n",
    "    train_errors, val_errors = [], []\n",
    "    \n",
    "    # train on 1 to m of the data, up to all of the data in the split off training set\n",
    "    for m in range (5, len(X_train)):\n",
    "        # fit/train model on the first m samples of the data\n",
    "        model.fit(X_train[:m], y_train[:m])\n",
    "        \n",
    "        # get model predictions\n",
    "        y_train_predict = model.predict(X_train[:m])\n",
    "        y_val_predict = model.predict(X_val)\n",
    "        \n",
    "        # determine RMSE errors and save history for plotting\n",
    "        train_errors.append(mean_squared_error(y_train[:m], y_train_predict))\n",
    "        val_errors.append(mean_squared_error(y_val, y_val_predict))\n",
    "        \n",
    "    # plot the resulting learning curve\n",
    "    plt.plot(np.sqrt(train_errors), 'r-', linewidth=2, label='train')\n",
    "    plt.plot(np.sqrt(val_errors), 'b-', linewidth=2, label='val')\n",
    "    plt.xlabel('Training set size')\n",
    "    plt.ylabel('RMSE')\n",
    "    plt.legend(fontsize=18)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first want to demonstrate an obviously overfitted model for this data.\n",
    "Fit a 50th degree polynomial to the noisy data using `scikit-learn` and `PolynomialFeatures`.\n",
    "You may want to start using `scikit-learn` pipelines here.  \n",
    "\n",
    "Visualize the fit performance by plotting the learning curves for your overfit 50th degree model.\n",
    "\n",
    "Report the intercept, coefficients, R^2 score and MSE/RMSE of the final fitted model.  Also visualize the fitted model you obtained on this data.  You should\n",
    "again plot the raw data, the fitted model and the true function.  You may want to abstract this into a function, as you are doing the same thing here you\n",
    "did before to create a plot, and will need to perform the same plot a few more times after this as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a pipeline here if needed for a degree 50 set of PolynomialFeatures that is then\n",
    "# trained with a standard LinearRegression\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the learning curves.  You may need to change the limits of your plot, because if the data is overfitting\n",
    "# the performance on the validation data may be very bad compared to on the training data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display intercept, coefficients and R^2 fit score here\n",
    "\n",
    "# determine and display MSE/RMSE fitted cost here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize your overfitted model again.  Show the noisy data, the fitted 50th order regression and the\n",
    "# true function as before\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a note of the RMSE you achieved for this model on all of the data it was trained with.  Now use the `x_test` and `y_test` data\n",
    "to make new predictions and calculate MSE and RMSE on the test data.  Observe the RMSE you get on the test data and compare it with what you saw for the\n",
    "training data.  You will write down your observations at the end of this part of the test, after fitting a model with some regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine MSE/RMSE on the test data here for your overfit 50th degree model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularization of Polynomial Regression\n",
    "\n",
    "As we did in assignments for this class and in our lecture notebooks, demonstrate\n",
    "fitting a model using regularization this time.  Continue using a degree 50 model, but use regularization to fight overfitting.\n",
    "Use Ridge ($\\ell_2$), Lasso ($\\ell_1$) or a combination of both with an Elastic Net.  Your goal is\n",
    "to demonstrate a model that gets about the same $R^2$ and RMSE score on all of the data as the overfit and 5th degree best fit models,\n",
    "while reducing (regularizing) the fitted parameters. You should be able to obtain a model where the theta parameters are much lower,\n",
    "and the plot of the fitted model will look smoother and closer to the true function.\n",
    "\n",
    "As with the previous step, show the learning curve of the model you select to at least demonstrate it doesn't appear to be overfitting as much.\n",
    "\n",
    "And also as with the previous steps, report the Intercept, Coefficients, $R^2$ score and MSE/RMSE of the model you demonstrate regularization on.\n",
    "\n",
    "Finally give a plot again of the noisy data, the fitted model and the true function on a single figure so you can compare with the previous \n",
    "when you discuss it below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a pipeline here if needed for a degree 50 set of PolynomialFeatures that is then\n",
    "# trained with a regression that uses regularization to avoid/reduce overfitting\n",
    "\n",
    "# plot the learning curves.  You may need to change the limits of your plot, because if the data is overfitting\n",
    "# the performance on the validation data may be very bad compared to on the training data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display intercept, coefficients and R^2 fit score here\n",
    "\n",
    "# determine MSE/RMSE cost of the fit on the training data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize your fitted model with regularization again here.  Show the raw noisy data, the fitted model\n",
    "# and the true function on the plot.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again for your model with regularization, run predictions on your `x_test` dataset and calculate the MSE and RMSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine MSE/RMSE fitted cost here on your test data for your regularized model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2 Discussion\n",
    "\n",
    "In the following markdown cell discuss the following:\n",
    "\n",
    "- Compare the $R^2$ score and RMSE reached on the training data for the 5th order fitted model, the 50th order overfit model and the model you used regularization on.\n",
    "- Also look at the RMSE you get on the held back test data for each of these.  What conclusions can you make from the change of RMSE from the training to the test\n",
    "  data here?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your written answer for Part 2 should go in this markdown cell.  Use complete sentences and discuss your observations\n",
    "of the trained models on this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Logistic Classification\n",
    "\n",
    "In this section you will generate another artificial data set that contains 5 discrete categories, thus we\n",
    "are going to perform a classification instead of a regression in this part of the assignment.\n",
    "\n",
    "This time, however, we will use one of the `scikit-learn` dataset generator methods to generate the data set for us.  You will\n",
    "then fit a logistic regression classifier to the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Artifical Multiclass Dataset\n",
    "\n",
    "There are many functions in the `sklearn.datasets` that can be used to \n",
    "[generate artificial datasets](https://scikit-learn.org/stable/datasets/sample_generators.html)\n",
    "in order to test out various machine learning methods.  The simplest one for generating\n",
    "labeled datasets suitable for classification tasks is the\n",
    "[`make_blobs` dataset generator](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_blobs.html).\n",
    "\n",
    "In the next cell, use `make_blobs` (it was already imported above) and generate an artificial dataset with 5 categories or\n",
    "labels.  You will need to read the documentation for the function to determine the parameters\n",
    "you need.  Make a dataset with 1000 samples and 2 features.  You will need to specify 5 as the\n",
    "number of centers of the generated blobs, in order to generate data with 5 categories.  Use\n",
    "a cluster standard deviation of 1.0.  Reset the random seed first to 42 in the first cell before where\n",
    "you call `make_blobs`, so that you will get the expected results when you generate you multiclass\n",
    "dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure we reset the random seed to 42 so that you get the expected results when generating your dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate your multiclass dataset with 5 classes.  There should be 1000 samples in the dataset with 2\n",
    "# input features.  Use a standard deviation of 1.0 for the cluster centers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you correctly create your dataset, then you should have an `X` feature matrix of size (1000,2) and a\n",
    "`y` label vector with (1000,) labels.  The labels will be the values [0 1 2 3 4] for this artificial\n",
    "multiclass dataset.  The first few items of the `X` inputs and the `y` labels should be as shown:\n",
    "\n",
    "```python\n",
    "> print(X.shape)\n",
    "(1000, 2)\n",
    "\n",
    "> print(y.shape)\n",
    "(1000,)\n",
    "\n",
    "> print(np.unique(y))\n",
    "[0 1 2 3 4]\n",
    "\n",
    "> print(X[:5])\n",
    "[[ 5.02007669  2.58375543]\n",
    " [ 3.23236714  1.195353  ]\n",
    " [-6.10792848 -9.72865221]\n",
    " [ 5.19966928  3.05395041]\n",
    " [ 1.38081864  4.5933741 ]]\n",
    "\n",
    "> print(y[:5])\n",
    "[1 1 2 1 4]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment these and run to verify that you have correctly generated your multiclass classification dataset\n",
    "#print(X.shape)\n",
    "#print(y.shape)\n",
    "#print(np.unique(y))\n",
    "\n",
    "#print(X[:5])\n",
    "#print(y[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next cell create a scatter plot of the artifical data set and use markers or colors to indicate\n",
    "the 5 classes.  You can use similar methods as we have shown previously in class lecture\n",
    "notebooks, but you need to extend the concept to display the 5 categories of this dataset.\n",
    "\n",
    "Your figure details can differ a bit, but it should look something like the following figure.  Here you are using color and/or shape to indicate each of the\n",
    "5 categories of the data.  The 2 features of the data should use the x and y axis respectively in your figure.\n",
    "\n",
    "You should find, using this seed and if you have your settings correct for `make_blobs()`, that most of the classes\n",
    "are pretty well separated (this is controlled by the cluster standard deviation).\n",
    "With the exception of classes 1 and 4, whose centers ended up having quite a bit of overlap,\n",
    "and thus these classes will be the most difficult to predict and separate for this data.\n",
    "You should, for example, expect that if you generate the confusion matrices of a fitted \n",
    "classification, these classes may have the most errors with one another.\n",
    "\n",
    "![Scatter Plot of Multi-Class dataset](test-01-question-03-blobs.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a scatter plot of the artificial multiclass dataset and use color/shape to visualize\n",
    "# the categories of the data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this classification you are going to evaluate the goodness of the model fit by doing a train/test\n",
    "split and evaluating the classification performance on the test data.  In the next cell, create `X_train`, `X_test`,\n",
    "`y_train` and `y_test` arrays from your artificial multiclass dataset, using a 80%/20% train/test split.  You should\n",
    "use the `scikit-learn` methods for splitting the data here that we have shown examples of and used in previous\n",
    "assignments and lectures.  Use a random_state of 42 to make sure that you split the data in the same way\n",
    "every time.\n",
    "\n",
    "If you use a 80/20 split, you should end up with 800 samples in the training data, and 200 in the testing data:\n",
    "\n",
    "```python\n",
    "> print(X_train.shape)\n",
    "(800, 2)\n",
    "\n",
    "> print(X_test.shape)\n",
    "(200, 2)\n",
    "\n",
    "> print(y_train.shape)\n",
    "(800,)\n",
    "\n",
    "> print(y_test.shape)\n",
    "(200,)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform a 80/20 percent train/test split of the artificial multiclass data here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit a Multiclass Logistic Classifier\n",
    "\n",
    "In this section you should fit/train a multiclass logistic regression instance using a `scikit-learn`\n",
    "`LogisticRegression` instance on your training data.  I will not tell you the exact parameters to use.\n",
    "Try and see in the next part if you can tweak the parameters to get good performance on the\n",
    "test data.  Try comparing using multi_class='multinomial' vs. multi_class='ovr' (one vs. rest).\n",
    "\n",
    "In the next cell, fit your model to the training data only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a LinearRegression instance and \n",
    "# fit a Linear Regression classification to the training data \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the model you just fit, show the predictation accuracy on the training data, and then on the\n",
    "held back test data, for your fitted model. (**Hint**: recall the `score()` method for\n",
    "`scikit-learn` model instances, which in the case of classification models returns the accuracy of the model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the prediction accuracy on the training data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now show the prediction accuracy for your model on the held back test data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should be able to get an accuracy of about 0.97 or better on the training data, and not much worse,\n",
    "if at all, on the test data most of the time if your regression is fitting correctly.\n",
    "\n",
    "Now in the next cell, display the confusion matrices on the trained data and on the test data for\n",
    "your logistic classifier.  As a hint, the `confusion_matrix()` method from `scikit-learn` can do this for you\n",
    "for both train and test data, if you have the labels and the predictions for each of these.\n",
    "\n",
    "You might want to confirm that class 1 and 4 are having the most confussion using your classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display training data confusion matrix here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display test data confusion matrix here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Decision Boundaries\n",
    "\n",
    "Using examples from class and your fitted logistic regression, visualize\n",
    "the decision boundaries that were learned by your multi-class logistic\n",
    "regression on the training data.  You should use countour maps for this visualization.\n",
    "Your figure should look similar (though it doesn't have to exactly\n",
    "reproduce) to the following example of the learned decision boundaires:\n",
    "\n",
    "![Multi-class Dataset Decision Boundaries](test-01-question-03-decisionboundaries.png)\n",
    "\n",
    "\n",
    "So try and visualize the decision boundary\n",
    "of your fitted logistic classifier here.  Since this is a multiclass classifier, it is somewhat difficult to find the\n",
    "decision boundary lines for each of the individual classifiers being used.  So the easiest approach is to use\n",
    "the `predict()` method form a mesh/grid of prediction values covering the 2 features, and make a contour plot\n",
    "of the resulting areas. I have given examples of doing this before, but again you would need to expand this\n",
    "for the multiclass case of 4 classes here.  You should see that, since this is a basic logistic regression, \n",
    "linear decision boundaries are being fitted by the model as best they can to make the classification decisions\n",
    "for the fitted model.\n",
    "\n",
    "Put your visualization in the next cell of the fitted decision boundaries of your classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display resulting decision boundaries of the fitted classification model using contour plot function here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
